# Image-caption-generation-with-CNN-and-RNN
Humans are able to create succinct descriptions of images by highlighting the key elements. Machine vision systems, on the other hand, describe the scene by using an image that is a two-dimensional array. The concept is to learn a mapping from the image to the sentences and map the image and captions to the same area. We show that an automatic technique can do this. A major issue in artificial intelligence that unites computer vision and natural language processing is how to automatically describe the contents of images. A model that can produce captions for images that are extremely relevant. With the aid of deep learning, an image caption generator can determine the context of the image and add the appropriate captions. A convolutional neural network (CNN) and a recurrent neural network (RNN) are the two main components of the architecture. This model can generate image captions that are typically grammatically accurate and semantically meaningful by acquiring knowl- edge from pairings of images and captions. For evaluation, the BLEU score metric is used and achieved a decent BLEU-1 score of 0.442616 on the test dataset.

# Data
In this project, I utilized the Flickr 8K data set to generate image captions. A range of images out from Flickr 8k data set show different situations and scenarios. Each of the 8091 images in the data set is paired up with five different captions to provide clear descriptions of important events and entities as shown in Figure 1. These images are of different dimensions. The data set is split into training, validation and testing respectively.

Text preprocessing is the process of cleaning up our text data so that it may be presented, analyzed, and predicted for our goal. Words or combinations of words can be reduced by omitting letters. Stop words like symbols, numeric values, single characters, punc- tuation’s, emojis, etc are removed. These words are then tokenized using the Keras built-in preprocessing python library. Tokenization is a process of splitting the sentence into smaller words. These words are then assigned a index to map the text into numbers. But the length of each caption varies, for instance, the dataset has maximum caption length of 34 and the minimum length of 2. Therefore, We added padding to ensure that all captions are having the same length. Padding is simply a process of adding layers of zeros to our input captions. We used keras built-in function, by default this is done by padding 0 at the beginning of each caption until each caption has the same length as the maximum caption. Finally, ”startseq” and ”endseq” tokens are added to captions to indicate start and end of the caption.

# Feature Extraction
Feature Extraction intends to decrease the number of features in a dataset by generating new features from the ones that already exist (and then discarding the original features). The majority of the information in the original collection of characteristics should then be able to summarize by the new, smaller set of features. Through the combination of the original set, a condensed version of the original features can be created. To extract features from the images, we utilized the VGG16 model. The final layer of the VGG-16 will be left out since we just require feature extraction and this layer is for object classification. Every single image in the dataset are used to extract the features. VGG-16 model extracts 4096 features from the 224 * 224 input images

# Model Architecture
The architecture for our model that is to be attached in the last layer of the VGG-16 model, includes both a deep convolutional neural network (CNN) to obtain image features and a recurrent neural network (RNN), more specifically LSTM, for label captioning. Essentially the CNN will act as the encoder and the LSTM will be the decoder.
  
That was chosen since the LSTM being a sequence prediction model would have difficulty in understanding the input images which have spatial structure. But if CNN extracts the necessary features from the images then the LSTM can work as usual.
